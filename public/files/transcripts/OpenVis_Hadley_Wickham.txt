The Role of Visualization in Exploratory Data Analysis
Hadley Wickham

>> All right.  I'm going to go ahead and welcome our last speaker to close us out.  I can't believe it's already been two days.  I'm superhonored to welcome Hadley to the stage.  If you have never used R, you nay have not heard of Hadley, but pretty much everyone else certainly has.  He's a chief scientist at our studio and a member of the R foundation.  You probably all know him from the tiny verse packages.  If you use R in modern daytimes.  You pretty much rely incredibly heavily on everything he's done.  And he is also a writer, an educator and a frequent speaker promoting the use of R.  I'm really delighted he could join us to talk about the amazing things he's doing.  Please welcome Hadley.
[ Applause ]
>> Thanks, Irene.  So I'm going to sort of fortuitously bookend this conference a little bit.  I think it's been great, this ideal of why are we creating visualizations?  We're not creating them to give the pixels a work out.  We're creating visualizations for some purpose.  And the purpose that I typically create visualizations for is data analysis.  So I'm going to talk a little bit about kind of the idea of explorer to data analysis and some of the other pieces you need to do that apart from visualization.  And this is a little bit of my journey.  Because I really started off like this is the first R package I created for visualization.
But it very quickly became apparent to me that, like, being able to create a good visualization is a result of a lot of successful steps.  You've got to be able to get your data into R, tie it in and arrange it.  And unless you can do that, you cannot create a good visualization.
And so I have the� this is my sort of mental model of data analysis tools, or data science tools that you have to start.   The first thing you can do, you have to do is, is you have to import your data from whatever crazy format it lives into your data analysis environment of choice.  Now that, for me, obviously, is R.  But I think this applies regardless of what you're using.  Then I think it's a really good idea to store your data into consistent format.  I'm not going to talk about this too much.  But I think if you do ever use R, I'm just learning about this idea of tidy data.  Incredibly powerful idea.  Because the idea is you put your data in this one format and leave it in that format.  You're not constantly ramming the output from one function to the input for another function.
Then there's sort of three main tools for actually understanding a dataset.  You're going to do some sort of transformations.  You might be doing simple things like counting, you might be just changing the order, you might be creating new variable on the functions of existing variables.  Simple stuff, but often very important.
And there are two main really big toolkits that are useful.  Visualization, which is obviously the focus of this conference.  I'm not going to talk too much about this.  But visualization is fundamentally a human activity.  This is making the most of your brain.  In visualization, this is both a strength and a weakness of visualization.  You can see something in a visualization that you did not expect.  And no computer program could have told you about.  But because a human is involved in the loop, visualization fundamentally does not scale.  So to me the complimentary tool to visualization is modeling.  I think of modeling very broadly.  This is data mining, this is machine learning, this is statistical modeling.  But basically, whenever you have made a question sufficiently precise, you can answer with a numerical summary, summarize with an algorithm.  And they are computational tools, which means they can scale.  As�the data gets bigger and bigger and bigger, you can keep up with that by using more sophisticated computational.  Just more computation.  But every model makes assumptions about the world.  And a model by its nature cannot question those assumptions.  So that means that at some fundamental level a model cannot surprise you.  That's why I think modeling and visualization are so complimentary.  Visualization can surprise you, but doesn't scale.  Models don't surprise you, but they scale.
So the vast majority of any real data analysis, any real data science project is going to involve doing these things again and again and again.
And then at some point, which I'm not even going to really touch on today, I'm going to be doing it to you, you have to communicate the results of your analysis.  Now visualization also has a really powerful role to play here as well.  This is a great way of getting those insights that are in your head into the heads of other people.  But I'm not going to talk about that today.
Today I want to kind of focus on this explore part of this.  This transforming this visualizing.  Not really into modeling today.  And I don't want to talk about this abstractly, I want to talk about this concretely.  So what I'm going to do is I'm going show you two examples.  And I was trying to think like what data is the most interesting data to show you?
And really, personally, I think the most interesting data is data about me.  So I'm going to show you two little data explorations that I have done recently for fun.  The first one we're going to kind of look at my patterns of GitHub commits.  And then we're going to look a little bit at places that I've traveled to.  And then finally we're going to kind of culminate them in this diagram where we look at my GitHub commits based on time of day and the time zone I'm currently at and whether or not I'm traveling.
So I'm going to do this with a live demo.  Not only is it a live demo� hopefully we have enough battery.  I am also using the development version of a number of packages and the development version of R Studio.  So wish me luck.  So let's� I'm going to load a bunch of R packages.  I'll talk a little bit later about some of these packages so you can at least look them up if you're interested, and I'm going to write a couple of helpers.  This is going to scrape the data from GitHub using GitHub's API.  And I'm not going to live that dangerously.  I have actually cached all this data locally.  I'm not going to download it live for you.
But what I'm doing, basically I've got to do this� to get the data I have to do it in two steps.  First of all, I have to ask GitHub, what are all the repos I've contributed to?  I've asked for what I'm reasonably certain is the last hundred repos that I have touched.  And then I'm going to go and get all of the commits to all of those repos.  Take a little bit longer.  And I've cached it locally.  This is a JSON file, a hierarchical structure.  I have a whole bunch of repos.  If I drill down into a repo.  Inside much that repo I have a whole bunch of commits and some information about each commit, which is the unique identifier and some information about the commit itself.  Like who committed it.  This one is not me.  It's Scott.  What it was about.  And somewhere in here.  Oh.  In here.  When that commit occurred.  So what I'm going to do is I'm going to do a little bit of scripting.  And I am going to do� that would be helpful, but it is the wrong plug.  I think we'll� I think we'll manage.  This is going to be like one of those horrifying� any time anyone ever posts a screen shot of their phone on Twitter, there's always like 2% battery left.  I like to live dangerously.  What I really like is a fantastic name for what I'm going to do next.  And there's rectangling.  So I'm going to turn this crazy hierarchical list into a rectangle.  A tidy rectangle.  Each of the columns is a variable and each of the rows is an observation.  So I'm going to end up� and I accidently pressed the wrong keyboard shortcut.  I'm running the script.  I'm going to end up with the data frame.  There's about 23,000 commits.  I have the repo, who authored it, the date, time, and so�on
And I've done tricky things here.  This data from GitHub has a date time.  So this is the exact instant in time that commit occurred.  And I'm going to partition this into two things, into a date and into a time.  And I'm going to do this partitioning thing in a really kind of tricky, or perhaps, well, very hackie way.  I'm basically going to say� I'm just going to convert all of the years, months, days, and seconds to exactly the same time.
So I'm going to sit� so I'm just going to have basically times on January�1st, 2016.  Which is basically� but this idea that I've got this variable.  I want to partition into two pieces is a really, really useful idea.  So done that.  The first thing you do whenever you've got some data from somewhere, it's a really good idea to take a look at what's in there.  So I just counted like the number of commits in each repo.  There are some surprises here.  So, for example, the spark repo.  I haven't done anything on that.  Would bed on DPLYR and a few of these other things.  That's okay.  But when we look at this, the count of authors, you'll see there's 22,000 commits here and only 4,000 of them from me.
So some of those GitHub API, there's no way to get just the data about me.  I've had to pull in this data about other people I couldn't care less about.  I'm going to get rid of those.  I'm going to do a little bit of filtering to look at the last years of data and arrange in a chronological order and look at that again.  And again, I have somehow managed to use the wrong keyboard shortcut.  Whoops.  And let's just take a look at that.
So 3,000 commits now, and you can see you've got the name of the repo, who authored it, that's always going to be me, the Xia, and you can see the commit.  Which is things like� do I need explicit slashes, I guess?  Fortunately, there's no cussing on this page.  The other thing I'm going to do here which I think is a great practice when doing a data analysis is dump this out to CSV.  And because I am using Git here, I can easily take a look.  That's interesting.  And you can see there is some massive did� whoops� massive diff there� which is� thanks.  Massive diff which is suspicious that the data changed so much the last time I ran this which was only a little while ago.  If you were doing a real analysis, you would say what is going on there?  I'm going to press on and hope everything is okay.
So one of the reasons I started looking at this data originally was I sort of wanted to try and feel like this is some selfreflection.  What is it that I'm actually been working on recently.  So the first plot I'm going to do is just a plot.  Again, if you have never seen R code before.  I guess how many of you do use R?  Cool.  So even if you've never used R before, hopefully you'll be able to make out what's going on here.  Because the structure of this code is designed to help not only communicate to the computer what's going on, but to communicate to other people.
Here I'm creating this data set.  The Hadley data set.  This is called a pipe.  This is basically just going to feed this data set into this step and it's the easiest to pronounce.  To take the Hadley data, then paper into ggplot.  And on the X axis I want the date, and the Y axis, the repo.  And the geom, that's short for geometric object.  Let's do a scatterplot first of all.  Just so you can see why a scatterplot is not very good.  Do a scatterplot here, run this code.
And, wow, you can't really see anything, which is part of the point.  But you see here, with a scatterplot, just because I'm just drawing points, those points ended up plotted on top of each other and it's hard to see like the relative density.  So what I'm going to do instead is use this quasi random.  And I'll show you on the slide a little bit later if you'd like to learn more about that.  But basically this adds a little bit of random noise and spreads those points out so they don't overplot so much.
So that's better.  There's still way too many.  This is showing every single repo I've contributed in the last year.  That's a huge amount of data.  The screen is not very high resolution and keeps flicking on and off.  But I can only fix one of those problems.  I'm going to lump together a whole bunch of those repos into an other category.
Really.  Just going to swap ports in the hope that that will make things slightly better.
[ Laughter ]
Okay.  Fingers crossed.  So what I'm going to do is I'm going to use this function called factor lump that's going to take this categorical variable and lump it together.  So I'm going 15 categories, dumping everything else in other categories.  Which is a very, very quick way of simplifying that data.  And maybe since this screen is so big, so small, I'll just jump it down to 10.  So now you can see the ten repos that I've contributed most to.  And the rest are all lumped into this other category.
Now, this is a step forward because we kind of got rid of the small-scale stuff, so they can see the big picture.  But how is the Y axis ordered?  Well, currently it's just ordered alphabetically.  Which is, I mean, it's okay, I guess.  But I think it would be more useful if I could order it so the things I have been working on most recently are at the top.
So I want to improve this visualization, right?  But to improve the visualization, I have to modify the underlying data.  So what I'm going to do is I'm going to reorder this to reorder the repo by the average date.  And then for various not that interesting reasons, that's going to end up with the most recent stuff on the bottom and certainly we learned yesterday from Matthew that maybe that makes sense for your culture, but for me, that looks weird.  So I am then going to reverse it.
So you can sort of read this pipeline as a series of imperative statements.  Take the repo, reorder it according to the average date.  Then reverse that ordering.  Oh, and then don't forget to lump together so you've just the 15 or the ten most common repos.
So now you can see kind of chronologically� I guess, yeah, going down through all those repos at the top, moving down to the things I have been working on most recently at the bottom.
And one thing you can kind of see here, I think, is that I'm pretty bursty.  You'll see that for a lot of these packages there's a whole bunch of commits and then nothing happens for a long time.  You might also notice� something's going on here.  We've got two packages, right?  Where it seems I have been working on them exactly at the same time.  Well, it turns out that this� if you want to artificially inflate your GitHub commits, it's very easy to do that.  It turns out these are actually the same commits, but this was a project that got split into two pieces.  And those kind of historical commits ended up in the same place.  So we could weed those out by looking at that unique identifier.  But I couldn't figure out a single algorithm to decide which repo that belonged to, so I left them in there.
The other thing I was interested in is introspecting on what is my working process like?  So what I'm going to do is I'm going to create a new variable.  Whoops.  No, I'm not.  I am just going to plot those two date/time variables.  I'm going to put date on the X axis.  There's the first argument.  And then the time of day on the Y axis.  And again, display that with this quasi random blob.
And this is� okay.  I think.  But I'm going to tweak it a little bit.  I'm just going to kind of round each day.  Like there's so many days in the X axis.  It's hard to see the fine scale detail.  So I'm just going to round this to a week.  So we're just kind of seeing what are the times that I work on code look like across multiple weeks.
And it looks like you can kind of see� make sure this is the right time zone that I'm in.  And you can see normally I don't do much work before 6:00�a.m. and much work after 6:00�p.m., although there are a few exceptions.  Right?
And this certainly was not me getting up at like 3:00 in the morning to program, this� my speculation was at this point without looking at anything else, I was in another time zone.  It wasn't that my working times changed, I had changed to a place where the times were different.  We'll come back to that a little bit later.
Then we could also say, well, what happened� what happens if we look at this by day of week rather than the date or the week itself?  As you can see, I'm kind of like� I'm not really a hobbyist programmer anymore, I'm like a professional programmer.  I just work when I get paid to work.
So most of the time I'm working on the regular days of the week.  Now, to create this plot, I have had to do some more data manipulation.  A new variable from the date time that gives me the day of the week.  And for reasons unknown to me, R has decided that Saturday is the first day of the week.  Right?  And that� well, I guess actually Sunday is the first day of the week and we're going to go from the bottom up.
So, again, to create a more natural visualization, I have to do a little data manipulation here.  Which I did have in here previously, but I have accidently deleted.  So what I'm going to do is I'm going to take this weekday and I'm going to use this function called shift which kind of just shifts the bottom to the top and then I'm going to reverse the order of that.  Just to create a more natural, to me, more natural order where the weekends are� the two days on the weekend are next to each other and Monday is at the top of the plot and time proceeds down.
So this is� like to create this visualization� this is a really simple visualization, right?  This is a scatterplot.  Most of the challenges to create this visualization are not in the visualization, it's getting the right data aggregated in the right way, taking this date/time thing and breaking it down into variables that are more informative, like the day of the week and the time of the day.
Now, what I wanted to do next� so when I look at this, right?  We identify most of the time I work between 6:00�a.m. and 6:00�p.m. and at least if you know me as well as I do, you would imagine that maybe I'm traveling on these times.  So the thing I wanted to look at next was my travel data.
And to be honest, the reason I originally looked at that travel data was laziness.  Because I recently decided to become a U.S. citizen.  And one of the things you have to do when you become a U.S. citizen is list all the times you have left the country for more than three days in the last five years.  And I was like, oh, my god, that's going to be awful.  But fortunately, I use TripIt to track my trips.  And there's an API.  Why don't I scrape it and use that API to get that data?  And that led to this part of the process which I evocatively call endless screaming.  Which is off of TripIt's API.  Then I carefully read the Web page, and discovered if I emailed and asked them nicely they could turn on basic auth.  So I'm going to do that and enter my password in a way you can't see it so you don't hack my Trip I know it.  And again, I have code.  So many modern Web APIs use JSON.  It's easy to slip it into there, and use a nested tree.  And write helper functions that aren't important and bring down all of that JSON.  Again, we can look at this.  Just to take a look.  So I've got this JSON file.  177 trips.  And if I open one of these trips, you can see, well, some� idea.  When was this?  This was from the 4th of April to the 6th of April.  And I went to the primary location was Detroit.  And if I drill down on that you can see I can even find out like the latitude and longitude of that trip.
So I'm just going to go through and I'm going to rectangle that, flattening that list so I have columns.  Like my identifier, the start date, the end date, the latitude, longitude, the city and the country.
And I've got to run this line.  So we can look at this and so you can see it didn't rerun this recently enough so you could see this trip.  But you can see the last trip was to Detroit and I travel a lot, obviously.
Again, I want to track what's going on with this data, so I saved that as a CSV.  I'm using Git, put that into Git.  And if my input data changes, I'm going to see this very easily in the diffs.  This is a great way of making sure data is not changing when I do not expect it to change.
Okay.  Then I thought, well, let's take a look at that data.  And so I'm going to do� what I thought would be a great way to visualize it is to put each country I visited on the Y axis and basically draw lines showing how long I was in that country on the X axis.  And like most initial visualization ideas, this turned out to be basically useless.  Because most of the time I'm visiting somewhere in the U.S. and a few times I visit� most of the time I visit one other country.  And like the only other country I visit most frequently is New Zealand.  The other thing when I looked at this plot that I was suddenly perturbed by is what country is NA?  It's like, I don't remember like that naught Africa?  But turns out this is a missing value.  NA is how R records missing values.  I'm sure this is some trip with TripIt.  I should check that out before I tell you a compelling story about the data.  So I'm going to say give me all the trips where the country is missing.  Yeah.  So you can see here there are four trips for whatever reason TripIt could not tell me where that trip went to.  Again, if you're doing real data process, you dive into this and figure out why.  But I'm just going ignore it and continue on.
Okay.  So I thought, then, well, maybe, what am I going to do next?  So then maybe it's too hard to show countries on the Y axis.  Let's use the Y axis for something more useful.  So I'm going to again do the same trick.  Why don't I put each year as a line and just have the days within the year.  And I'm going to use my same trick here to kind of find the start day.  I'm just going to define like the day of the year.  Like 1st of January or 5th of February.  I'm going to set the year to 2010.
So just kind of� so they all have the same year.  And I'm going to repeat that.  This time I'm drawing a Sigmund, it has four parameters, it has the starting exposition, the day my trip started.  The ending exposition, the day my trip ended.  And then I'm going to make these flat lines so both the starting and the ending Y position will be the year.
And when I do this, again, I get a little visualization a little bit.  But there's still some problems.
And you notice there's like a couple of years where it looks like I spent the whole year traveling.  I was like, that's a little odd.  But it turns out my problem is that this approach is not a very good approach.  Right?  Because if I left on December 1st and came back on February�1st, this is going to end up kind of switching the things around.  A line gets drawn in the wrong direction.  So, again, if I was doing a real analysis or something that was higher stakes, I would go back and figure out how to do that correctly.  But for now I'm just going to throw those trips away.
And I think that� I mean, you're laughing a little bit and it's not a great thing in general.  Being able to do that, saying this is important, but I'm not going to worry about it now.  That's a great technique to just ignore it and move on.
So we'll skip a few steps since I'm going a little slower than expected.  I'm going to make it a little fatter.  And then I'm going to color them by country.  Again, this is not very good because the� you are not capable� first of all I cannot fit like 20 countries on my legend.  And then you can't perceive the differences between the country colors.
But we could� just a starting place.  These are all� that's a terrible visualization because the legend is taking up most of the space.
But, again, like so many of these challenges are not about the visualization, but about the data.  And sometimes the visualization makes me think, well, I need to change this in the data and then sometimes the data is just things I want to visualize.
I only have a couple of minutes left, so I'm going kind of skip to the conclusion.  And this� I will not bore you with more endless screaming that this entailed.  But I found an API that goes from a latitude and longitude point and temps you the time zone at that location.  And then figuring out how you compute the local time given a date, time, and even location is a little tricky.  So I wanted to show one other visualization quickly here.  And this is just a visualization� the other thing that visualizations are great for, and I know Mike is a big believer, is visualizations are good to check that you have programmed something correctly.  And the fact that each of these colors as a time zone on a straight diking a follow line indicates that I have coded this correctly.  Which is awesome.  And then I can� whoops.  Skip to this very� skip to this plot.  The colors are now a little redundant.  So I'm going to get rid of that.
And now this is sort of like a morph an explanatory graphic, communication graphic.  So I've added some labels and stuff.  And this is basically� this is my life.  When I am not traveling I am incredible consistent on GitHub, a Git committer.  I basically commit between 6:00�a.m. and 6:00�p.m. every day.  You can see there are two big gaps on Tuesday and Thursday.  I do to the take very long lunches on those days.  That's when I do yoga.  And you can see I hardly ever practice� hardly every code on the weekends.  But when I'm traveling, well, again, I tend to not code before 5:00�a.m. and I don't� well, I code quite late at night.  Much, much more variation because my daily rituals are now gone.
So two cool packages if you use R that I wanted to point out.  One which I didn't get a chance to show you.  Ggrepel, has the layout stuff from D3.  You can make sure your labels are not on top of each other.  And this GGB Swarm package that provides packages, great way of doing dots where the dots don't overlap.  This was a quick overview.  Many of you have seen R code before.  Hopefully I have not persuaded you not to use R again.  If you want to learn anything I talked about, all of the packages I showed today, many of the packages are in the tidy verse.  If you would like to learn more about it, it's got a Website and I have a book about it with Garrett Groman, R for data science.  That or any book from O'Reilly, use this discount code, rfd, 40% off the physical book and 507% off of the�book.  Thank you.
[ Applause ]
