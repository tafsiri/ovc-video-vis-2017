Visualizing Data with Deck.GL
Nicolas Belmonte

>> Hey.  Hey, everybody.  Let's wrap up that break.  Get to your seats.  Wherever those seats may be.  You're trying to swap out seats.  Play musical chairs.  Give you all a minute.  Get your coffee.
Cool.  All right.  A little bit more shuffling.  But such quiet shuffling.  I'm so impressed.  Thank you.  Awesome.  All right.  Well, thanks for coming back.  Not surprising after these morning talks.  Am I right?  Amazing!
[ Applause ]
Cool.  All right.  Well, I'm excited to welcome our next speaker, Nicolas Belmonte.  He's spoken at OpenVis before, two years ago.  Blew minds away talking about WebGL.  Runs the Vtach visualization team at Uber.  And before that, works at Twitter and made visualizations of Twitter data for the public.  And he's going to join us to talk about Deck.GL.  An Open Source library from him and his team.  So please welcome Nicolas.
>> Can you guys hear me?  Yes.  Yeah.  Hello, everyone, my name is Nicolas Belmonte.  Run the visualization team at Uber.  Today I'm talking about Deck DL many version 4 opinion announcing it right now.  Hey, what happened?  Why didn't you release it before?  I join Uber, and people in design or data science.  They worked with other teams to solve problems.  You can imagine visualization or even, like, partnering with a selfdriving car.  Kind of a team to kind of have this 3D immersive visualizations that I cannot show which are really cool.  We started using deck.GL, used for data visualization and abstract visualization use cases.  And we decided to open source end of last year.  So I'm going to talk to you a little bit more about this framework.
So what is Deck.GL, a largescaled data visualization library.  You may have datadense visualizations.  That doesn't mean that you need to render a million, you know, points on a screen.  You can use WebGL today to do also data processing and data manipulation and aggregation and so on.  So this framework attempts to basically help you with that.  But to render a thousand elements or 10,000 or a hundred or a million.  That can be done with this.  Decided to go with WebGL.  There was a new major version released a few weeks ago.  WebGL 2.  It allows for a lot of interesting, you know, general purpose like GPU programming type of stuff and we're planning to kind of conquer that space with Deck.GL.  So, again, high performance GPS computing.  Uber manages millions of points every day.  We do this with maps.  It does something very specific very well and plays well with other libraries.  You can hook it with Reacted or hook it here with map box GL which provides perspective mode.  We use Map GL as a base layer and over that layer with others Deck.GL.  You can see how they're synchronized.  And you can see that it follows this instancing and layering paradigm.  You might know this from D3.  This data binding.  You're creating a scatterplot.  Of an array of points of elements and map them one to one to circles.  So what you do with that selection, it's instancing a bunch of circles and then you can tweak them by switching or changing a few attributes.  WebGL 2 is good at that.  It's instancing and you can have a cone or a sphere, whatever, a circle.  And you can instance many of those with only one call.  And so you'll get basically a very efficient way to do this data binding.  So we follow this pattern which is interesting from the influence perspective, but also from the WebGL performance type of perspective.  And then the lay erring paradigm comes from GIS.  You have a map and multiple layers.  Could be land and water, another one could be labels, roads, et cetera.  And you can now have layers on top of that display data.  In this case we have, you know, a scatter plot on the top left, a hexagonal on the left, and an arc layer on the bottom right.  It's a good way to look at the data and plug into the framework if you wanted to develop your own layers.  But we provide like a dozen different layers that do these things for you.  As I mentioned before, the layers are not purely visual stuff.  Not your only rendering stuff.  But you can also perform different things like aggregation and filtering.  So massaging the data in different ways that, you know, all happen within the layer as well.  So you can imagine this as well as combining D3 with Deck.GL.  And you can pass in the sites and uses the D3 to render the polygons and renders with the polygon layer inside.  This is a way of adapting code and doing some data processing in the process.  What we're looking at here is a hexagonal layer and filtering by attributes and percentile.  You can remove the upper percentile of the distribution.  We use it for abstract data visualization.  I mentioned we do some of the visualization stuff as well.  Here we're visualizing the output of an embedding using Tesnee.  This is stuff that I cannot tell you what it is.  And it's very insightful.  And the purple stuff is especially interesting if you know what it is.
[ Laughter ]
Finally, something we're really proud of that we have worked on here with this framework which is WebGL has a limitation, quote, unquote, which is, as you are coding your shading language, it only sports like 32bit of precision floats.  So basically, there's only so much you can get from it.  And if you were using GPS, global positioning systems and wanted to get it to the centimeter level, for example, WebGL would not enable you to support that.  There's a lot of things that maps do like using Web workers and changing systems and stuff like that to be able to support that.
We took a fully different approach.  Basically, it's a brute force approach.  We decided to emulate 64bit operations by basically doing a technique where you pack 32bit floats into a vector two.  And we had to redefine all math operations, linear and nonlinear in the shader to support 64bit.  So what that means, when I show this very trippy image on the left, you have the 32bit position.  It tells you how much you can go.  And then on the right one, you can see that you can go exponentially further.  Right?  All that is emulated into the GPU.  Now that you can where it is and how you can be using it, how does it work?  If you open up the black box, this is how it looks.
Probably if we start from the left side, we have this React component, it has some data.  You want to render it using Deck.GL.  To render on top of a map, it's the high-level components that we have.  Deck.GL, wrap GL, a wrap friendly wrapper.  And the lower level library to the rendering which is internal, mostly.  So you will not need to play with it that much.  So you're passing the data through the Web comet point.  It's interesting with WebGL, the camera system is combined.  So as you pan, zoom, tilt the map, both the layers and the map are synchronized.  And then as you pass your data, imagine you have a layer manager.  You have a layer with three sub layers.  Imagine you want to render a geoJSON, right?  Pass in the geoJSON to a layer.  And the layer says this feature is a point.  So I dedicate that to the scatter plot.  This is a polygon, this is a line, delegate this to the line layer.  You can kind of have composite layers in that way.  But if you wanted to simplify your example here, we could just have one layer, that's the scatterplot layer.  And it takes care of the rendering layer.  Hopefully that's clear.
So what is React-Map-GL, it's a Reactfriendly wrapper around mapboxGLjs.  It's amazing and you should totally use it.  And React map GL provides a user friendly interface on top.  It can be a div or a canvas.  And this is really where deck.GL comes in.  You can create layers to use that context and to make it really performant.  And luma.GL, decided to go with this.  Sometimes there's differences around vision and framework for open source.  Some want to invest in VR and AR stuff, which is fine and fun.  But then things like adding Web GL support get deprioritized.  So we wanted to have control on that.  So we developed luma.GL.  It's cool, it's written using ES6 code base, and WebGL one and two entities.  If you want to get familiar with the lower level stuff, reading the source code of luma.gl is good.  So you can use instancing, feedback and so on.
All right.  So that's the overall architecture.  How would you use this?  What are the main use cases?  So there are three approaches.  I guess they get more complex as we go down on that list.  But on the right, you get to see some of the core layers.  This is an actual screenshot of the layers.  So we have more than a dozen core layers and some of them can be quickly adapted to your needs.  So one of those use cases could be using existing layers, pretty straightforward.  Another one that's pretty straightforward is adapter layers, which we'll visit.  And if you wanted to go more and tweak some shader code, et cetera, you could extend these layers to have them customized.�on the first point, existing layers, Deck.GL, there are examples, standalone examples.  You can clone them, copy them, tweak them, learn from them.  You can view the code.  For example, there's a fat arrow there.  You click there, it will take you to the GitHub page.  And then as you look at the code, you'll see a bunch of props.  You can see the API docs, all interactive and figure out the props, what do they do, basically?  In this case, we're instancing the layer.  And then there's the code inside of the larger piece of code.  We're wrapping that around a Deck.GL overlay which instanced it as part of the reverse statement and adds that in.  And as part of Deck.GL we add that as part of the map.  So in this case we have the map with the layer inside of it.  On the extending layers, as we� as I mentioned before, you might want, for example, to write your own shaders for a layer.  You might want to add uniforms when joins a layer to add extra functionality.  Base layer, arc lay, arc brushing, add filtering or something like that.  You can do that with the arc brushing layer.  And finally, if you want to go more mild and have your own custom picking mechanism or have full flexibility into what WebGL can do without this framework bothering you, you can do that by extending the layers as well.
An example of this is� I don't know how useful this example is, but it's a good illustrative example.  You have the scatterplot layer, renders a bunch of circles.  You want them to be rounded rectangles.  So in this case we extend the scatterplot layer and have the draw function.  With the draw function, we have the same as before and pass in the new uniform.  Think of uniform as a prop, and it's a corner radius, right?  And then on the shaders, instead of passing the same shaders as before, since we're adding a new functionality, we'll pass in our own custom shadow shader.  And we'll use the same vertex shader as before.  And set up the corner props for the radius there.
Basically, this is a fragment shader.  I'm not going to go into much detail.  In order to create a circle, it discards a bunch of pixels being rendered on the screen.  You can do that from the corner radius prop.  Uniform.
And then finally composite layer.  As I mentioned in the architectural diagram, you can manage multiple primitives into one layer.  GeoJSON is cool for that.  It breaks down a complex rendering problem into smaller, testable classes that you can use.  But you know you can also use layers to do data processing or aggregation.  The rendering, talked about the hexagonal layer.  And does it before the rendering.  The layer that I just exemplified, pass in a bunch of sites, compute using D3.  Once you have the polygons, you can render them.  That's a way of adapting a layer, changing the interface so it suits better your specific domain and problem.  Yep.  So those are pretty much kind of the use cases where we see people kind of like plugging into the framework.
So for that we audited a lot of the APIs.  Made sure they were consistent across multiple players and provide good documentation for the specific use cases.
So� the first year at Uber I started coding.  I went back to coding for this example.  And I realized why I stopped coding.
[ Laughter ]
But it's an interesting use case.  It's a way of like, you know, creating custom layers to visualize wind.  In this case, the data for the wind is not correct.  We do some data processing from sparse data coming from different kind of wind stations to have like a more uniform pattern so we can render this.  But I wanted to showcase that because I think it's an interesting way of using WebGL to do just data processing and not the rendering itself.
So the story for this example, two years ago I was here and I showed a wind map as well.  And last year Patricio who used to work at Maps take the same data set and also created a visualization of wind using Time Gram, an awesome tool.  So I wanted to do a reremix of this.  It, you know, the data is already cleaned up.  It's easier.
so something that really bothered me about the data set, it's sparse.  You have all these points.  They encode wind velocity, direction, temperature and elevation.  But, you know, they're discreet points, they're not interpolated.  How can we interpolate this data so no matter where you are in the U.S., you can get an approximation of what the wind information is?
So use triangulation.  So basically, what we do is we can encode wind speed into a color channel.  Imagine, like, red.  And angle to green.  And� yeah.  Speed or temperature on blue.  And so you can render those dots as colors, right?  Into an image.  And every color will encode this threedimensional data.
So what do you do with intermediate points.  If you have that as a triangle and you're rendering through, you know, WebGL, it's really fast they need to interpolate all the values in between.  You can take advantage of that to do that for all your data sets.  So the only thing that we need to do first is the Delaunay triangulation for the stations in the U.S.  This is the first time I'll use the Delaunay triangulation for something that's pretty.  And once we do that, you will get all that interpolated, you can sample any pixel in the image and you will get the interpolated wind value.
So the step after that was getting a vector field to show up.  So for that we created a layer, used an instancing paradigm, right?  So we have a bunch of lines.  They get cloned multiple times.  And that way we can basically have an overlay of how the wind looks at every point in the U.S.  And this is the first kind of approximation of it.  So we used the Delaunay triangulation to encode elevation.  We use it as yet another layer.  We have a layer for the stations, a layer for the Delaunay triangulation, and a layer for the vector field.
So I use that.  A little bit of lighting and stuff like that during the process.  And this is the first stage.  So on the left you have a slider.  So these are the last 72 hours of wind.  So you can see that vector field kind of interpolate.  On the top right, you can see the outlier of wind, like Mount w where wind is exponentially stronger than anywhere else.  So don't go there.
And then at the bottom you get to see some of the stations, right?  So every yellow dot there is a station.  But we don't care about them anymore.  Because we're interpolating all of the data.
So the next thing that I decided to use was transform feedback.  And this is a new technique that� well, a new feature that WebGL 2 has in it�I'm not going into detail with the rendering pipeline.  But previously, from the left to the right.  So you pass a bunch of points, imagine 3D coordinates as an array.  It goes through the vertex sheer, which is programmable.  And you can have transformations to the points, translate them, rotate them, scale them, do stuff like that.  And there are multiple other processes.  From the points, you create triangles.  From the triangles, you have the 3D concept, and rasterize it.  And it goes through the fragmenter, and after rasterization, it decides which pixel to add to the screen.  The Vertex Shader, and you can go through a lot of points and sometimes you don't need to renter the stuff in the end.  You can do random math.  WebGL has a thing, transform feedback, we can use the data from a vertex shader as a buffer.  And you can imagine yourself using an assigned function in a vertex shader and passing in an a random amount of points and getting that back.  There is a little bit of overhead of creating the buffer and sending it in.  But overall superfast and made parallel.
So this is how it looks like.  There are some voices in Florida.  So we throw particles per million randomly in the U.S.  And we sample the place where those particles are moving around.  And we kind of update the velocity and the position of that particle.  And then as you change the time, you'll just keep sending over particles.  Every particle has a time to leave.  So, you know, they'll move around for� I don't know� a few seconds.  Then I'll take it back, throw them at some other random place.  So, of course, what's interesting is, there are interesting patterns, like you have finer granularity on top of your data and the way it's being visualized, I think it can be reused in multiple ways.  We will add these layers as core layers in a later version of Deck.GL.  You can throw in anything, build your own layers and create an app with it.
So taking all the credit, but this is a Deck.GL team.  As I mentioned before, we do visualization in business insights, advanced analytics and core visualization work more centered around computer graphics.  This team has been working on that.  It started as an internship project by one there, and now it's core to a thousand different internal labs at Uber.  That's pretty powerful.  Thanks.
[ Applause ]

I'll be around.  I would rather escape Q&A.  Thanks.
>> Thank you so much, Nicolas.
[ Applause ]  
