Text Mining and Visualization, The Tidy Way
Julia Silge

>> We're going to introduce our next speaker.  Julia, come on up.  Julia is a data scientist at Stack overflow.  Her work involves analyzing and modeling complex data sets and communicating about technical topics with diverse audiences.  If you have not seen her blog, it's a delight to read.  She covers a lot of interesting ground.  Lots of art tutorials.  I have learned a lot from her.  I'm excited she could join us today.
>> Thank you.
[ Applause ]
>> You're welcome.
>> Great.  Thank you all so much.  I am really happy to be here.  And what I'm going to be talking about today is text mining and natural language processing and how we can get from text mining and raw text that we need to analyze to interesting visualizations.  But I'm going to be talking how to do this� these text mining task was that we need to do from a particular perspective.  And that's from the perspective of using tidy data principles and applying� and using tidy tools that we can use here.
And the point that I want to make here today with you� the argument that I want to try to make is that applying tidy data principles can make text mining easier.  Can make the analysis section easier.  Can make our move from analysis to visualization easier.  And by easier, I mean, easier to reason about.  The things that we want to do.  And easier to integrate the kind of tasks this you want to do into workflows that are already in wide use by data analysts and data scientists.  So we have a lot of resources about how to go about doing them. and more effective.
So the work I'm talking about is based on an R package called tidy text.  I'm one of the authors of this package along with my collaborator, Dave Robinson.  So this work is centered within the R package ecosystem of tidy verse tools that includes tools� includes packages like DPLYR, tidy R, Broom, and ggplot2.  And the point of the tidy package is to provide function and supporting data sets so analysts and data scientists and data journalists can approach analyzing text with this same infrastructure, the same powerful infrastructure that has proven to be very effective for people to be able to do data manipulation, data visualization.  To be able to apply the same infrastructure to extend it to text analysis.
So I know that in this room there's probably a variety of people's familiarity with what I mean when I say the phrase "Tidy data" from the main developer of these packages who is sitting somewhere here, to people who are probably unfamiliar with what I mean when I say, "Tidy data."  So let's talk through just� I'm not going into a super esoteric definition what have I mean.  But what I mean when I say tidy data, specifically in the context of text.
So how is text stored?  We all produce a lot of text, we use words a lot.  But how is text stored when we bring it into a computer and we want to analyze it?  Often text is stored, you know, if you first read it into R or any programming language, you're going to store it as a string in R.  And we often call it a character vector.  If you have a whole bunch of them we can put them together and we can annotate them with some information like, say, the title or the author or something.  And we can put them together in what we call a corpus.  Or we might store text information in something called a document term matrix.  And so this is a sparse matrix that has a row for every document.  A column for every term, which is usually a word.  And so there's a lot of zeros in this matrix, but there will be a value that tells you where every document has a word.  These are all examples of data structures for storing information about text within the context of using code to analyze text.
But none of these are a tidy data structure in the context in which I'm going to talk about it today.  So let's talk about a tidy data structure.  So the big takeaway that I want you all to think about when you think about tidy data structure in the context of text, is that we're going to have one observation per row.
And an observation in the context of text is the observational unit of text that you are interested in.  Oftentimes that means a single word.  That's a word.  Doesn't have to be a word.  Depending on the analytic question that you're asking, that could be something like an N gram or a sentence or something like that.  But let's think about one observation per row as being the thing that you're interested in studying, like, say, a word.  So let's step through a quick small example using part of a poem by Emily Dickenson who wrote some lovely text in her time.  So let's look at a little bit of an example here.
So these are four lines from one of Emily Dickenson's poems.  And here we're assigning them it to a character vector in R called text.  So if I look at what's stored in text, it's a character vector to.  I can load the DPLYR package and add another�column that's going to tell me what line.  So what line of the poem are we on here?
This text is now in a data frame, but it's not yet in a tidy format data frame.  So let's load the tidy text package, and let's use the token tokens to transform the data from an untidy data structure into a tidy data structure.  What this function is going to do, instead of each line of a poem on one row, we're going to have single words on a row together.  So the observational unit is single words.  So we want to have one observation per row.  Which in this case means one word per row.
So notice that a couple of things about this data frame that we have after we did this process.  We still have the other columns that we set up before.  So in this case that means we still know which line of the poem each one of these words came from.  And removed the lower case and removed the punctuation.  This is the best choice for analysis moving forward.
So that was a little bit of a tiny, tiny bit of text.  Now let's look at a bigger section� a bigger chunk of text.  Let's take the six completed published novels of Jane Austen and work on converting these to a tidy text for that.  So in R you can access the completed text of Jane Austen's novels in the Jane Austen R package.  Yes, yes.  That's one of my packages.
[ Laughter ]
So we can� let's load the Jane Austen R package as well as DPLYR and string R and set up a data frame using the code like this.  So the output is a data frame like this.  So we have a couple of columns.  The text column contains the actual text content of the books, and then book, line number, and chapter are annotating each row of text.  So that tells us what book, what line of the book, which chapter.  So line number and chapter will restart when we get to a new book.  So we have a data frame, but we don't yet have a tidy data frame.  So let's use unnest tokens to do that.  We don't have a text column anymore now.  We have a word column.  Also notice that the data frame is longer.  We have a lot more rows.  Which is what we would expect here.  And notice we have the other information we had.  With�no�which book, chapter and line each one of these words came from.  So congratulations, we did it.  We converted our text to a tidy format and I'm very proud.  We did it.  This is very exciting.
But why might we be interested in this approach?  Why would� why am I talking about applying tidy data principles to text analysis?  The reason why this is an approach that is powerful is because there are text mining tasks that we often need to do that become natural extensions of a common tidyversed operations.  For example, often if you're doing a text analysis you need to remove stop words.  Stop words are words that are very common in a language and are not interesting for an analysis.  For example, in English, these are words like the, and, of, to.  And you can remove stop words using DPLYR's antijoin.  You can easily remove them.  And we can ask the question, what are the most common words that Jane Austen used in her novels?  And then we can just simply take one more step further and make a visualization for that and ask� and see, what were the most common words that Jane Austen used in her novels?  We see words like miss, time, dear, lady and sir.  And we also see some proper names here of characters.  We see Fannie, Emma, and Elizabeth.  So we're able to get to a visualization that communicates the results of our analysis with just a handful of lines of DPLYR and ggplot2 code because we used tidy data principles with our text.
This is an example of an analysis that's based on just frequencies.  Just counting, basically.  Another analysis that I did within the past year that is based on a similar approach.  Just a countingbased analysis is one of using a dataset of pop lyrics.  So there's a dataset out there of pop lyrics of songs that were on the billboard's hot 100.  The year-end Hot 100 from the 1960s to the present.  And I asked the question, what states are mentioned more often in pop lyrics?
So I took a similar approach where I took the raw text.  I transformed it to a tidy data format.  And then I could make a visualization to answer the question.  Which states do we see that are mentioned more often?  So this map was made with ggplot2.  And we can see that states like California are mentioned most often.  The New York ones are, of course, mentions of New York City, not New York State here.  So we can see that a state like California is mentioned most often.
But, of course, California's a very populous state.  So maybe we would prefer to divide by the population of each one of these states.  And instead look and see what states are mentioned in pop lyrics most often relative to how many people are there.  So here we see that states like Hawaii and Montana are mentioned in pop lyrics very often relative to how many people live there.  So these are states that have big impact in pop music and culture relative to how many people actually are there.
So this is an example of a kind of analysis that is possible using these kind of tidy data tools and moving through a pipeline from raw text to visualization using a pipeline of DPLYR and ggplot2 that's well understood and wellsupported and gives us a visualization that I'm proud of and that I can share.
This was picked up by the Washington Post wonk blog a couple months ago.  And I was excited to see this being shared around.
So that was what is tidy data and how can we do analysis based on counting and frequency.  But the next thing I want to talk about is sentiment analysis.  So sentiment analysis is a kind of analysis that asks the question of text, what is the emotional content of text?  Or what is the opinion� the opinion that is being expressed in text?  And often sentiment analysis is done with sentiment lexicons or dictionaries.  And these are lists of words that are assigned some scores.  So they might be scored from negative 5 to positive 5 saying like how negative of a word or how positive of a word is this?  Or they might be scored in a binary fashion.  This is a negative word.  This is a positive word.  This is a joy word.  This is a sadness word.  And the way you assess the sentiment of a section of text is by adding up the sentiment of all the words that make up the text there.
If you use text data that is� that you're storing in a tidy data format, you can implement sentiment analysis with an inner join operation.  So back to Jane Austen.  We have Jane Austen in a tidy data format and we use an interjoin with one of the sentiment lexicons, afterward we have the words that were in both datasets and we can add them up.  We can use just a couple more lines of DPLYR and tidy R code here actually.  And then the data frame that we have at the end of that, what it actually shows is, how does the sentiment change during the narrative arcs of these six novels?  So we can see� and during what parts of the books do we see more positive words being used or more negative words being used?  We're able to get to this visualization that I made with, again, with ggplot2 because the data is in a tidy data format.  And then I can use inner join to implement a sense of analysis.
So the sections here of, for example, extended negative sentiment or positive sentiment correspond to plot entitles that we, as human readers, understand to be good and bad things to do.  So we're seeing real effects and real insights by doing something like this.  For example, if you look at "Pride and Prejudice," that section near the middle of the book where you see that first extended text of really negative sentiment there.  That's where Mr.�Darcy proposed for the first time so badly and Elizabeth is upset and angry with him.  And three quarters through, a really extended section of negative sentiment.  Lydia elopes with Mr.�Wickham.  So touch a terrible scandal.  So we're able to see these real� we're able to identify plot events that are reflective of our understanding� our human understanding of what this text is saying by embracing tidy data principles and making visualizations in this way.
It's also important when we're doing sentiment analysis to be able to understand which words are contributing to each sentiment.  And when we are keeping our data in a tidy data format, it's easy to get at this.  For example, in here we're doing the inner join again and counting, but instead of counting up� only adding up sentiment, we're going to count by word and sentiment.  And then we can see how much is each word contributing to sentiment.  So you can see which were the negative words, which were the positive words.  And we can, again, make visualizations to get a better understanding of what's going on here.  Here you can see on the positive side, these words all look good, right?  We see these are the Jane Austen words like happy, love, good, great, pleasure, happiness.
And some of those words on the negative side are fine.  Poor, doubt, impossible, afraid, scarcely.  But this top word, miss, in Jane Austen, that's used as a title for a young, unmarried woman.  This is where the sentiment lexicon has misidentified a word in the text that's a neutral word.  It's important that we explore the words that are driving it.  And exploring what's going on with the sentiment analysis is important.  So in this case we could add "Miss" to a custom list of stock words, use antijoin to remove it�and see how that changes the results.  This is some of the power of applying tidy data principles.  It is easy to iterate, it's easy to keep going in this process because these pipelines are effective and easy to reason about here.
So far, we have talked about what is tidy data when it comes to text.  We have talked about sentiment analysis.  And the last big thing I want to talk about is how can we programmatically understand what a document is about?  So we might say, okay, we're going to find out what a document is about.  Let's look at the words that are used the most often.  So let's look at term frequency.  So term frequency is when we take� we divide the total number of words used in a document� we take the number of times a word is used and we divide it by the total number of words in a document.  Okay.  A document uses a word a lot of times.  And that is an important word in that document.  But that, again, we run into the problem of some words being very common, but not important.  A more sophisticated way to approach that problem is something called inverse document frequency.  So inverse document frequency is a weight.  So it is� it works like this.
It's the natural log of a ratio.  And so let's say we have a collection of documents.  And let's say all of the documents contain a word.  So then this ratio will be one, and then the natural log of one is zero.  And so that will get weighted down.  That will be� that will become zero.  But let's say we have our collection of documents and only one or a few of the documents contains a word.  Then that ratio will be bigger than one, and then the natural log will be a bigger number.  So our IDF, our inverse document frequency here, will be a bigger number.  So if you take term frequency and you multiply it by inverse document frequency, you end up with something that's called TFIDF.  And it's a statistic.  It's something you measure about a word in a collection of documents.  And it's a heuristic quantity, which means that people don't really know why it works or have any great theoretical backing for why it works, but what it is meant to do is that it identifies words that are common, but not too common.
So it's going to find you words that are common to one document within a collection of documents.  So let's look at how this works.  So let's go back to Jane Austen, naturally.  So first let's look at term frequency.  So we can actually just easily calculate term frequency by� from scratch using DPLYR verbs.  And we get a data frame that looks like this.  So for every book and word, how many times is the word used and how many words does the book have in total in it? We can make a visualization of this and see what is the distribution of N divided by total.  That's exactly what term frequency is.  N divided by total.  You'll notice these are very similar.  These distributions are very similar to each other and we see that there are a lot of words that are used just a few times.  And then we have these very long tails of there are just a few words that are used many, many times.  So those are the words� the words like the, and, of and to in the long tails.  What it's going to do is look at the distribution and find the words that are important for each document compared to the other documents.  So the tidy text package has an implementation of TFIDF.  It's bind TFIDF.  And the output will be a data frame like so.  So notice we have a TF now� that's the same value we have before.  Dividing in by total.  Notice here that IDF and, thus, TFIDF, are zero for these examples.  And that's because all six of Jane Austen's novels contain the words thee, to, and of.  If we arrange it, sort it so that we look at the things with the highest TFIDF, then we see these words.  And if you're familiar with Jane Austen's novels, you know who these people are.  And even if you're not familiar with Jane Austen's novels, you can tell these are proper nouns.  These are the names of people and places in Jane Austen's novels.
So what does that mean that we've learned?  We have gotten to a visualization like this using tidy data principles, and we were able to get there and learn that in the corpus of Jane Austen's novels, she used similar words.  Her word choice was similar from one novel to the other.  And the thing that distinguishes the most one novel from the other were the� are the proper nouns.  The proper nouns are what make the novels most different from each other in a linguistic sense.  So we're able to see that because of the way we've applied these tools.
Another application of TFIDF I want to talk about is related to the NASA Datanauts program that I've been with the last year.  It's involved with data science learners and data science mentors.  And one of the projects I have been involved in is one to understand NASA datasets.  So they have over 32,000 dataset at NASA and have metadata at the data sets.  What subsection of NASA did they come from?  What's the title?  What's the description?  Keywords tagged?  NASA wants to understand the connection between the datasets in better ways using machine learning techniques.  I have been involved in topic modeling and other various techniques.  But I applied TFIDF to the description fields in just the same way that I talked about in the last example.  And the results look like this.
So this is a subset of the NASA datasets.  So if you look at NASA datasets tagged with the keyword seismology, the high TFIDF words are like risk, earthquake, acceleration and hazard.  But look at NASA datasets tagged with the keyword budget, you see different TFIDF words.  You see the Office of Management and Budget, or fiscal and financial.
So using a measuring TFIDF for words and description fields got us to meaningful insight with two very different kinds of text.  We had narrative fiction from several hundred years ago.  And we have technical description fields on datasets.  And in both cases we got real insight because this is a flexible and powerful technique.
So one thing I'm going to mention briefly is that so far in everything I have talked about today, the observation level was the single word bipartisan but that is certainly not the only observation level that we might be interested when it comes to text.  We can also look at things at the Ngram level.  Like, for example, if you look at bigrams like pairs of words with text, you can then ask questions about networks of words.  You can look at questions about how is negation affecting your sentiment analysis?  You can ask very interesting and complex questions by moving from the single word level to larger sections of text.  And this is all possible using the� still within approaching your data using the tidy� using tidy data principles.  I'm just going to throw up something else from the NASA data analysis.  This is a network of tags from the NASA dataset.
So this is a network of how often� or how correlated are different tags together?  So there are some tags that are actually correlated together with a correlation of one, like they always come together.  And then there's� then the correlation goes down from there where some tags are used, you know, less often together.  But we're able to make a network and see how� like what is the universe of tags at NASA on the datasets that are there and just try to understand this.  So this is possible� this kind of analysis is done within this whole universe of tools.
The� it is not necessary or required that text data is kept in a tidy data structure through all stages of an analysis.  This lets you have a, for example, a workflow where you might preprocess, filter, do initial exploratory data analysis using tidy data tools.  And then cast to a� say a document term matrix.  And then you can do some machine learning algorithm on the document term matrix, because it's something that involves like the linear algebra of the matrix.  And then you can tidy the output of your statistical modeling procedure and use Broom and ggplot2 to look at the output of your modeling procedure.  So tidy data principles with text is powerful and fun.  And a great way to use tools that are in wide� that are in wide use.  But it's not mutually exclusive with other data structures or other approaches when they also can be used.
So this� my collaborator, Dave and I, are writing a book on this.  So what this book sets out to do is to provide background and examples for people who want to apply this approach to the text analysis that they want to do.  So this book is available in its entirety online at tidy text mining.com.  It's also available for preorder on Amazon.
So it has chapters that have introductory material and ways to get started.  And then the last couple chapters are case studies.  They're beginning to end case studies with realworld actual data and how do we start?  How do we read in the data?  Or how do we clean it?  And then how do we do things like sentiment analysis or topic modeling?  And how do we gain insight from it?  And so there are whole beginning to end case studies there.  So we are happy to have this for people who want to use this approach to text mining.  And with that, I will say thank you very much.
[ Applause ]
